{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Test Analysis, Question 3\n",
    "\n",
    "After reading the problem a few times, it looks like we can organize the information with the following:\n",
    "1.  The baseline can be interpreted as a result we typically expect, if we hadn't performed any variations to the way the information flow is carried out between the customer and provider. In other words, the baseline is the results we obtain, on average, if no manipulations to the process were made. \n",
    "2. The quotes are meaningless in absolute terms.  Let's put the quotes in relative terms, comparing them with the total number of viewers.  Therefore, let's generate a new metric, called 'Success Ratio' (SR) that represents the ratio of quotes to viewers.  \n",
    "3.  The statistical question that deems appropriate is: are the variations drastically affecting results?  In other words, are their success ratios  statistically different from what we normally expect, AKA the baseline?\n",
    "\n",
    "So, let's first get started by extracting the data from the baseline and variations, cleaning them up as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "path = 'data.csv'\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "with open(path, newline = '') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        x.append(row[1])\n",
    "        y.append(row[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quotes', '32', '30', '18', '51', '38']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Views', '595', '599', '622', '606', '578']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [(int(x[1]), int(y[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 595)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = [(int(x[2]), int(y[2])),(int(x[3]), int(y[3])),(int(x[4]),int(y[4])), (int(x[5]), int(y[5]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, 599), (18, 622), (51, 606), (38, 578)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the success ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_ratio(input):\n",
    "\tsr = []\n",
    "\tfor i, v in input:\n",
    "\t\tsr.append(float(i/v))\n",
    "\treturn sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value = success_ratio(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05378151260504202]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure we all understand what the SR is saying, it can be interpreted as the following: on average, around 5.4% of provider actively respond to notifications, assuming we did not run any manipulations to the way the information flow is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05008347245409015,\n",
       " 0.028938906752411574,\n",
       " 0.08415841584158416,\n",
       " 0.0657439446366782]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation_set = success_ratio(variations)\n",
    "variation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the SR's of the four manipulations that were carried out.  The question to ask now is: are these SR's statistically significant? In other words, are they purposefully generating desired results, or are they just tiny variations from the expected result?  \n",
    "\n",
    "If we assume that the variations were all conducted independently, in isolated events, and if we also assume that the baseline is generated from a normal distribution, then we can run a one-sample t-test here.  \n",
    "\n",
    "A One Sample T-test allows us to work with small sample sizes to determine whether the manipulations that were made in the variation set are, on average, statistically \n",
    "significant from the expected value. \n",
    "\n",
    "So, let's break the problem down into two hypotheses:\n",
    "\n",
    "Null hypothesis:  Variations do not impact the SR\n",
    "Alternate hypothesis:  Variations significantly impact the SR\n",
    "\n",
    "Depending on the t-statistic that we obtain from the data, we can figure out whether to reject or not reject the null hypothesis.  If we can reject the null hypothesis, then perhaps the variations are purposefully giving us something we want.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=0.2942705909906809, pvalue=0.7877376338259627)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "onesample_results = scipy.stats.ttest_1samp(variation_set, expected_value[0])\n",
    "onesample_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our t-statistic is ~0.294, and pvalue is ~0.788.  \n",
    "\n",
    "The p-value is quite high, and definitely something to look at.  If we average out the variation set, we get:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05723118492119102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(sum(variation_set)/len(variation_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What the p-value is saying is that 79 out of 100 times we can expect to see an average of SR's to be greater than or equal to 5.7%, the average SR from the variation set.\n",
    "\n",
    "If almost 80% of expectations are above our variation set, is our variation set anything special then?  Well, let's look at the t-statistic.  Our t-score is 0.294.  In order to interpret this result, we have to calculate the degrees of freedom of our sample sets: 1 + 4 - 2 = 3.  Let's also set our alpha, which is a metric that determines how confident we are, to 0.10.  We want a one-tailed t-test, since we are only concerned with moving the SR in one direction, i.e. we want to increase it. \n",
    "\n",
    "![picture](t-test.png)\n",
    "\n",
    "If you select the row where df = 3, and select the column where one-tail = .10, then you'll find that the t-test = 1.638, which is the threshold you must pass in order to deem an experiment as statistically significant.  Since our t-score is 0.294, which is less than 1.638, we can conclude that we can't reject the null hypthosis.  In other words, our variations didn't generate results important enough to claim that they made significant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points/Questions to consider:\n",
    "\n",
    "Just because we obtained results that weren't desirable does not mean we should stop making experimenting.  Few reasons why:\n",
    "\n",
    "1.  The size from both samples is too low.  If we had more data, we would be able to generate results that would be much richer and conclusive than what we have now.\n",
    "2.  We are making the assumption that the baseline was generated as the average result.  If this assumption is wrong, then our t-test would be invalid, as it won't accurately depict how off-set the variation set is from the mean.\n",
    "3.  I'm curious what the methodology was under variation 3, as that generated a relatively high SR (8%).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
